ðŸ“Œ Objective

Implement and understand the KNN algorithm for classification problems, explore different values of K, evaluate model performance, and visualize decision boundaries.

ðŸ›  Tools & Libraries

Python

Pandas â€” Data handling

NumPy â€” Numerical operations

Matplotlib & Seaborn â€” Visualization

Scikit-learn â€” KNN model, evaluation metrics, preprocessing

ðŸ“‚ Dataset

Iris Dataset (from sklearn.datasets)

Features: sepal length, sepal width, petal length, petal width

Target: Species (setosa, versicolor, virginica)

ðŸš€ Steps Implemented

1. Load Dataset â€” Iris dataset from sklearn

2. Feature Normalization â€” StandardScaler to scale features

3. Train-Test Split â€” 80% training, 20% testing

4. Experiment with K â€” Checked accuracy for K = 1 to K = 15

5. Train Final Model â€” Used best K from experiments

6. Evaluate Model â€” Accuracy, confusion matrix, classification report

7. Visualize Decision Boundaries â€” Plotted boundaries for first two features

ðŸ“Š Results

* Best K: Found from accuracy vs K plot

* Accuracy: ~96% (may vary slightly depending on split)

Observations:

* Too small K â†’ High variance (overfitting)

* Too large K â†’ High bias (underfitting)

* Normalization is critical for KNN performance

ðŸ“· Visuals

Accuracy vs K plot

Confusion matrix heatmap

Decision boundary plot for first two features
